{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations on word vectors\n",
    "\n",
    "Welcome to your second programming assignment of week 1. You are going to apply operations between word vectors in order to assess their workability and usefulness. In the last part, you are going to remove the bias hurting these word vectors. \n",
    "\n",
    "**After this assignment you will be able to:**\n",
    "- Apply word vectors to operations like “king” - “queen”\n",
    "- Code and understand cosine similarity\n",
    "- Understand word analogies\n",
    "- Encode concepts such as \"gender\" in a vector\n",
    "- Debias word vectors using a series of projections\n",
    "\n",
    "Let's get started! Run the following cell to load the packages your are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from w2v_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also load the word vectors from an embedding matrix that maps every english word (from a wide vocabulary) into a vector. In this notebook, we chose to use 50-dimensional GloVe vectors to represent our words. Run the following cell to load the `word_to_vec_map` which contains all the vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0x93 in position 3136: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4bde853e0d27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_to_vec_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_glove_vecs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/glove.6B.50d.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Workspace\\local_jupyter_notebook_project\\deeplearning.ai\\Assignment5\\week2\\1. Word Vector Repressiontation\\w2v_utils.py\u001b[0m in \u001b[0;36mread_glove_vecs\u001b[1;34m(glove_file)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mword_to_vec_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m             \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mcurr_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0x93 in position 3136: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "words, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've loaded:\n",
    "- `words`: list of all the words from the vocabulary.\n",
    "- `word_to_vec_map`: dictionary mapping words to their GloVe vector representation.\n",
    "\n",
    "You've seen that one-hot vectors are not comparable in term of word meaning. Now you have word representations and the question is: how do you define similarity between two word vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Cosine similarity\n",
    "\n",
    "We need a way to find the similarity between vectors and this can be done using Cosine Similarity. Mathematically, for two vectors $u$ and $v$,\n",
    "\n",
    "$$\\text{CosineSimilarity(u, v)} = \\frac {u . v} {||u||_2 ||v||_2} = cos(\\theta) \\tag{1}$$\n",
    "\n",
    "where $u.v$ is the dot (or scalar) product of two vectors, $||u||_2$ is the magnitude of the vector $u$, $\\theta$ is the angle between $u$ and $v$. This similarity depends actually on the angle between $u$ and $v$ as you can see on the figure below.\n",
    "\n",
    "<img src=\"images/cosine_sim.png\" style=\"width:800px;height:250px;\">\n",
    "<caption><center> **Figure 1**: The cosine of the angle between word vectors is a good metric to define their similarity</center></caption>\n",
    "\n",
    "**Exercise**: Implement the function `cosine_similarity()` to evaluate similarity between word vectors.\n",
    "\n",
    "**Reminder**: The magnitude of $u$ is defined by $ ||u||_2 = \\sqrt{\\sum_{i=0}^{n} u_i^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: cosine_similarity\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    \"\"\"\n",
    "    Similarity metric defined by the formula above\n",
    "    \n",
    "    Arguments:\n",
    "    u -- a word vector of shape (n,)\n",
    "    v -- a word vector of shape (n,)\n",
    "    \n",
    "    Returns:\n",
    "    cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
    "    \"\"\"\n",
    "    \n",
    "    distance = 0.0\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Compute the dot product between u and v (≈1 line)\n",
    "    dot = np.dot(u, v)\n",
    "    # Compute the L2 norm of u (≈1 line)\n",
    "    norm_u = np.sqrt(np.sum(u**2))\n",
    "\n",
    "    # Compute the L2 norm of v (≈1 line)\n",
    "    norm_v = np.sqrt(np.sum(v**2))\n",
    "    # Compute the cosine similarity defined by formula (1) (≈1 line)\n",
    "    cosine_similarity = dot / (norm_u * norm_v)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_to_vec_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-303ac601e369>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfather\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_to_vec_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"father\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_to_vec_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"mother\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mball\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_to_vec_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ball\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcrocodile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_to_vec_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"crocodile\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfrance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_to_vec_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"france\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_to_vec_map' is not defined"
     ]
    }
   ],
   "source": [
    "father = word_to_vec_map[\"father\"]\n",
    "mother = word_to_vec_map[\"mother\"]\n",
    "ball = word_to_vec_map[\"ball\"]\n",
    "crocodile = word_to_vec_map[\"crocodile\"]\n",
    "france = word_to_vec_map[\"france\"]\n",
    "italy = word_to_vec_map[\"italy\"]\n",
    "paris = word_to_vec_map[\"paris\"]\n",
    "rome = word_to_vec_map[\"rome\"]\n",
    "\n",
    "print(\"cosine_similarity(father, mother) = \", cosine_similarity(father, mother))\n",
    "print(\"cosine_similarity(ball, crocodile) = \",cosine_similarity(ball, crocodile))\n",
    "print(\"cosine_similarity(france - paris, rome - italy) = \",cosine_similarity(france - paris, rome - italy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(father, mother)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         0.890903844289\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(ball, crocodile)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         0.274392462614\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(france - paris, rome - italy)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         -0.675147930817\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you got the correct expected output, please don't hesitate to modify the expected output cell above to test the cosine similarity with your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Word analogy task\n",
    "\n",
    "In the word analogy task, we complete the sentence <font color='brown'>\"*a* is to *b* as *c* is to **____**\"</font>. An example is <font color='brown'> '*man* is to *woman* as *king* is to *queen*' </font>. Mathematically, we are trying to find a word *d*, such that the associated word vectors $v_a, v_b, v_c, v_d$ are related in the following manner: $v_b - v_a$ is most similar to $v_d - v_c$. This can also be written as finding the word *d* such that $v_d$ is most similar to the \"combined vector\" $v_b - v_a + v_c$.\n",
    "\n",
    "**Exercise**: Complete the code below to be able to perform word analogies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: complete_analogy\n",
    "\n",
    "def complete_analogy(w1, w2, w3):\n",
    "    \"\"\"\n",
    "    Performs the word analogy task as explained above.\n",
    "    \n",
    "    Arguments:\n",
    "    w1 -- a word, string\n",
    "    w2 -- a word, string\n",
    "    w3 -- a word, string\n",
    "    \n",
    "    Returns:\n",
    "    best_word --  the word such that best_word is most similar to  w2 - w1 + w3\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert words to lower case\n",
    "    #w1, w2, w3 = w1.lower(), w2.lower(), w3.lower()\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Get the word embeddings v_a, v_b and v_c (≈1-3 lines)\n",
    "    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    words = word_to_vec_map.keys()\n",
    "    max_cosine_sim = -100                # Initialize max_cosine_sim to the minimum possible cosine similarity\n",
    "    best_word = None                   # Initialize best_word, it will help keep track of the word to output\n",
    "\n",
    "    # loop over the whole word vector set\n",
    "    for w in words:\n",
    "        \n",
    "        # to avoid best_word being w1, w2 or w3, pass them.\n",
    "        if w in [word_a, word_b, word_c] :\n",
    "            continue\n",
    "        \n",
    "        ### START CODE HERE ### (approx. 4 lines)\n",
    "        # Compute cosine similarity between the combined_vector and the current word\n",
    "        cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c)\n",
    "        \n",
    "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
    "        # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word\n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to test your code, this may take 1-2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'word_a' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-32ac8068a57d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtriads_to_try\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'italy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'italian'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'spain'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'india'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'delhi'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'japan'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'man'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'woman'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'boy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'large'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'larger'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'small'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtriad\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtriads_to_try\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'{} -> {} :: {} -> {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtriad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomplete_analogy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtriad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-6862f04ca405>\u001b[0m in \u001b[0;36mcomplete_analogy\u001b[1;34m(w1, w2, w3)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# convert words to lower case\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m#w1, w2, w3 = w1.lower(), w2.lower(), w3.lower()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mword_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_b\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_c\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m### START CODE HERE ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'word_a' referenced before assignment"
     ]
    }
   ],
   "source": [
    "triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('large', 'larger', 'small')]\n",
    "for triad in triads_to_try:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **italy -> italian** ::\n",
    "        </td>\n",
    "        <td>\n",
    "         spain -> spanish\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **india -> delhi** ::\n",
    "        </td>\n",
    "        <td>\n",
    "         japan -> tokyo\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **man -> woman ** ::\n",
    "        </td>\n",
    "        <td>\n",
    "         boy -> girl\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **large -> larger ** ::\n",
    "        </td>\n",
    "        <td>\n",
    "         small -> smaller\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you got the correct expected output, please don't hesitate to modify the expected output cell above to test your own analogies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Debiasing word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise, you will look at the implications of using a particular training dataset. You will first compute a vector $v_1 - v_2$, where $v_1$ represents the word vector corresponding to the word *woman*, whereas $v_2$ corresponds to the word vector corresponding to the word *man*. The resulting vector encodes the concept of \"gender\".\n",
    "\n",
    "The code below encodes the meaning of \"gender\" in a vector by taking the difference between word vectors of \"woman\" and \"man\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gender = word_to_vec_map['woman'] - word_to_vec_map['man']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will consider the cosine similarity of different words with the constructed *gender* vector. Consider what a positive value of similarity means vs a negative cosine similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print ('List of names and their similarities with constructed vector:')\n",
    "\n",
    "# girls and boys name\n",
    "name_list = ['john', 'marie', 'sophie', 'ronaldo', 'priya', 'rahul', 'danielle', 'reza', 'katy', 'yasmin']\n",
    "\n",
    "for w in name_list:\n",
    "    print (w, cosine_similarity(word_to_vec_map[w], gender))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, female first names have a positive cosine similarity with our constructed *gender* vector while male first names have a negative cosine similarity. This is not suprising and it is not a bias. Let's try with other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Other words and their similarities:')\n",
    "word_list = ['lipstick', 'guns', 'science', 'arts', 'literature', 'warrior','doctor', 'tree', 'receptionist', \n",
    "             'technology',  'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer']\n",
    "for w in word_list:\n",
    "    print (w, cosine_similarity(word_to_vec_map[w], gender))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice anything surprising? It is astonishing how these results underline the real-life existing bias between women and men. For example, \"computer\" is closer to \"man\" while \"literature\" is closer to \"woman\".\n",
    "\n",
    "The dataset you choose to train your word vectors on has immense power, so you should be careful when you train! You will now remove the gender bias of some of these words.\n",
    "\n",
    "Note that some words such as \"actor\"/\"actress\" or \"grandmother\"/\"grandfather\", should remain gender specific while other words such as \"receptionist\" or \"scientist\" should be neutralized, i.e. not be gender-related.\n",
    "\n",
    "You have to treat these two type of words differently when debiasing.\n",
    "\n",
    "### 3.1 - Neutralize bias for non-gender specific words\n",
    "\n",
    "The following figure should help you visualize what neutralizing does.\n",
    "\n",
    "<img src=\"images/neutralize_kiank.png\" style=\"width:800px;height:300px;\">\n",
    "<caption><center> **Figure 2**: The word vector for \"receptionist\" represented before and after applying the neutralize operation. </center></caption>\n",
    "\n",
    "**Exercise**: Implement `neutralize()` to remove the bias of words such as \"receptionist\" or \"scientist\".\n",
    "\n",
    "**Reminder**: a vector $u$ can be split into two parts: its projection over a vector-axis $v_B$ and its projection over the axis orthogonal to $v$:\n",
    "$$u = u_B + u_{\\perp}$$\n",
    "where : $u_B = \\frac{u . v_B}{||v_B||_2 ||v_B||_2} * v_B$ and $ u_{\\perp} = u - u_B $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize(word, bias_axis):\n",
    "    \"\"\"\n",
    "    Removes the bias of \"word\" by projecting it on the space orthogonal to the bias axis. \n",
    "    This function ensures that gender neutral words are zero in the gender subspace.\n",
    "    \n",
    "    Arguments:\n",
    "    word -- string indicating the word to debias\n",
    "    bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Select word vector representation of \"word\". Use word_to_vec_map. (≈ 1 line)\n",
    "    e = word_to_vec_map[word]\n",
    "\n",
    "    # Compute e_biascomponent using the formula give above. (≈ 1 line)\n",
    "    e_biascomponent = np.dot(e, g) / np.square(np.linalg.norm(g)) * g\n",
    "\n",
    "    # Neutralize e by substracting e_biascomponent from it \n",
    "    # e_debiased should be equal to its orthogonal projection. (≈ 1 line)\n",
    "    e_debiased = e - e_biascomponent\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return e_debiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gender' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-21e0ed5ca611>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"receptionist\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgender\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cosine similarity between \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mword\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" and gender, before neutralizing: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_vec_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"receptionist\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgender\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneutralize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"receptionist\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgender\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gender' is not defined"
     ]
    }
   ],
   "source": [
    "word = \"receptionist\"\n",
    "bias = gender\n",
    "print(\"cosine similarity between \" + word + \" and gender, before neutralizing: \", cosine_similarity(word_to_vec_map[\"receptionist\"], gender))\n",
    "\n",
    "v = neutralize(\"receptionist\", gender)\n",
    "print(\"cosine similarity between \" + word + \" and gender, after neutralizing: \", cosine_similarity(v, gender))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **cosine similarity between receptionist and gender, before neutralizing:** :\n",
    "        </td>\n",
    "        <td>\n",
    "         0.330779417506\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **cosine similarity between receptionist and gender, after neutralizing:** :\n",
    "        </td>\n",
    "        <td>\n",
    "         -3.26732746085e-17\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Equalize bias for gender-specific words\n",
    "\n",
    "Now, you will debias gender specific words using a technique called equalization.\n",
    "\n",
    "Equalization is applied to pairs of words which should differ in meaning only because of their gender properties. For example, \"businessman\" and \"businesswoman\" should have the same vector representation in the space orthogonal to the *gender* space. They should differ only in the *gender* space.\n",
    "\n",
    "Equalizing can be carried out in 6 steps as explained in the figure below.\n",
    "\n",
    "<img src=\"images/equalize_kiank1.png\" style=\"width:800px;height:300px;\"> <br>\n",
    "<img src=\"images/equalize_kiank2.png\" style=\"width:800px;height:300px;\"> <br>\n",
    "<img src=\"images/equalize_kiank3.png\" style=\"width:800px;height:300px;\">\n",
    "<caption><center> **Figure 3**: The 6 steps to carry out equalizing in order to debias gender-specific words. </center></caption>\n",
    "\n",
    "**Exercise**: Implement equalize()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize(pair, bias_axis, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Debias gender specific words by following the equalize method described in the figure above.\n",
    "\n",
    "    Arguments:\n",
    "    pair -- pair of strings of gender specific words to debias, e.g. (\"actress\", \"actor\") \n",
    "    bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender\n",
    "    word_to_vec_map -- dictionary mapping words to their corresponding vectors\n",
    "\n",
    "    Returns\n",
    "    e_1 -- word vector corresponding to the first word\n",
    "    e_2 -- word vector corresponding to the second word\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Select word vector representation of \"word\". Use word_to_vec_map. (≈ 2 lines)\n",
    "    w1, w2 = pair\n",
    "    e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2]\n",
    "\n",
    "    # Step 2: Compute the mean of e_w1 and e_w2 (≈ 1 line)\n",
    "    mu = (e_w1 + e_w2) / 2\n",
    "\n",
    "    # Step 3: Compute the projections of mu over the bias axis and the orthogonal axis (≈ 2 lines)\n",
    "    mu_B = np.dot(mu, bias_axis) / np.sum(bias_axis**2) * bias_axis\n",
    "    mu_orth = mu - mu_B\n",
    "\n",
    "    # Step 4: Use equations (7) and (8) to compute e_w1B and e_w2B (≈2 lines)\n",
    "    e_w1B = np.dot(e_w1, bias_axis) / np.sum(bias_axis**2) * bias_axis\n",
    "    e_w2B = np.dot(e_w2, bias_axis) / np.sum(bias_axis**2) * bias_axis\n",
    "\n",
    "    # Step 5: Adjust the Bias part of e_w1B and e_w2B using the formulas (9) and (10) given above (≈2 lines)\n",
    "    corrected_e_w1B = np.sqrt(np.abs(1-np.sum(mu_orth**2))) * (e_w1B - mu_B)/np.linalg.norm(e_w1-mu_orth-mu_B)\n",
    "    corrected_e_w2B =np.sqrt(np.abs(1-np.sum(mu_orth**2))) * (e_w2B - mu_B)/np.linalg.norm(e_w2-mu_orth-mu_B)\n",
    "\n",
    "    # Step 6: Debias by equalizing e1 and e2 to the sum of their corrected projections (≈2 lines)\n",
    "    e1 = corrected_e_w1B + mu_orth\n",
    "    e2 = corrected_e_w2B + mu_orth\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return e1, e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarities before equalizing:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word_to_vec_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-4cae4d36dfbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cosine similarities before equalizing:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cosine_similarity(word_to_vec_map[\\\"man\\\"], gender) = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_vec_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"man\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgender\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cosine_similarity(word_to_vec_map[\\\"woman\\\"], gender) = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_vec_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"woman\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgender\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_to_vec_map' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"cosine similarities before equalizing:\")\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"man\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"man\"], gender))\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"woman\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"woman\"], gender))\n",
    "\n",
    "print()\n",
    "u1, u2 = equalize((\"man\", \"woman\"), gender)\n",
    "print(\"cosine similarities after equalizing:\")\n",
    "print(\"cosine_similarity(u1, gender) = \", cosine_similarity(u1, gender))\n",
    "print(\"cosine_similarity(u2, gender) = \", cosine_similarity(u2, gender))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "cosine similarities before equalizing:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(word_to_vec_map[\"man\"], gender)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         -0.117110957653\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(word_to_vec_map[\"woman\"], gender)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         0.356666188463\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "cosine similarities after equalizing:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(u1, gender)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         -0.700436428931\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(u2, gender)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         0.700436428931\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Please feel free to play with the above cell to equalize your own pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also encourage you to run your implementations to tackle other types of bias such as:\n",
    "- quantity, which can be encoded using: \"numerous\" - \"single\"\n",
    "- reality, which can be encoded using: \"real\" - \"fake\"\n",
    "- wealth, which can be encoded using: \"poor\" - \"rich\"\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "Congratulations on finishing this assignment. Here are the main points we would like you to remember:\n",
    "\n",
    "- Many operations such as cosine similarity or analogies can be applied to word vectors.\n",
    "- Cosine similarity is the main metric to compare word vectors, although L2 distance may also be used.\n",
    "- Your word vectors are learned by training a model on a dataset, they thus suffer from a bias inherent to the dataset.\n",
    "- There are different debiasing methods given some words are bias-specific (need to be equalized) while others are non-bias-specific (need to be neutralized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**:\n",
    "- Bolukbasi et al., 2016, [Man is to Computer Programmer as Woman is to\n",
    "Homemaker? Debiasing Word Embeddings](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
