[ti:]
[ar:]
[al:]
[by:¾Å¾ÅLrc¸è´ÊÍø¡«www.99Lrc.net]
[00:00:02] You now know pretty much all the building blocks of building
[00:02:72] a full convolutional neural network.
[00:04:50] Let's look at an example.
[00:07:33] Let's say you're inputting an image which is 32 x 32 x 3, so
[00:12:43] it's an RGB image and maybe you're trying to do handwritten digit recognition.
[00:18:85] So you have a number like 7 in a 32 x 32 RGB initiate trying
[00:24:39] to recognize which one of the 10 digits from zero to nine is this.
[00:30:49] Let's throw the neural network to do this.
[00:32:79] And what I'm going to use in this slide is inspired,
[00:35:82] it's actually quite similar to one of the classic neural networks called LeNet-5,
[00:41:10] which is created by Yann LeCun many years ago.
[00:43:94] What I'll show here isn't exactly LeNet-5 but
[00:47:68] it's inspired by it, but many parameter choices were inspired by it.
[00:53:02] So with a 32 x 32 x 3 input let's say that the first
[00:58:52] layer uses a 5 x 5 filter and a stride of 1, and no padding.
[01:04:77] So the output of this layer,
[01:08:24] if you use 6 filters would be 28 x 28 x 6,
[01:13:73] and we're going to call this layer conv 1.
[01:18:80] So you apply 6 filters, add a bias, apply the non-linearity,
[01:23:66] maybe a real non-linearity, and that's the conv 1 output.
[01:28:35] Next, let's apply a pooling layer, so
[01:32:67] I am going to apply mass pooling here and let's use a f=2, s=2.
[01:40:28] When I don't write a padding use a pad easy with a 0.
[01:44:10] Next let's apply a pooling layer, I am going to apply,
[01:48:89] let's see max pooling with a 2 x 2 filter and the stride equals 2.
[01:54:97] So this is should reduce the height and
[01:57:06] width of the representation by a factor of 2.
[01:59:61] So 28 x 28 now becomes 14 x 14, and
[02:04:13] the number of channels remains the same so 14 x 14 x 6,
[02:10:47] and we're going to call this the Pool 1 output.
[02:15:53] So, it turns out that in the literature of a ConvNet there are two
[02:20:11] conventions which are inside the inconsistent about what you call a layer.
[02:25:56] One convention is that this is called one layer.
[02:30:91] So this will be layer one of the neural network, and now the conversion
[02:35:90] will be to call they convey layer as a layer and the pool layer as a layer.
[02:40:98] When people report the number of layers in a neural network usually people just
[02:45:22] record the number of layers that have weight, that have parameters.
[02:49:02] And because the pooling layer has no weights, has no parameters,
[02:53:04] only a few hyper parameters, I'm going to use a convention that Conv 1 and
[02:57:41] Pool 1 shared together.
[02:59:01] I'm going to treat that as Layer 1, although sometimes you see people maybe
[03:03:55] read articles online and read research papers, you hear about the conv layer and
[03:08:44] the pooling layer as if they are two separate layers.
[03:11:70] But this is maybe two slightly inconsistent notation terminologies,
[03:16:78] but when I count layers, I'm just going to count layers that have weights.
[03:22:05] So achieve both of this together as Layer 1.
[03:25:61] And the name Conv1 and Pool1 use here the 1 at the end also
[03:30:82] refers the fact that I view both of this is part of Layer 1 of the neural network.
[03:37:96] And Pool 1 is grouped into Layer 1 because it doesn't have its own weights.
[03:42:66] Next, given a 14 x 14 bx 6 volume, let's apply another
[03:47:58] convolutional layer to it, let's use a filter size that's 5 x 5,
[03:53:18] and let's use a stride of 1, and let's use 10 filters this time.
[03:58:79] So now you end up with, A 10 x 10
[04:04:35] x 10 volume, so I'll call this Comv 2,
[04:09:78] and then in this network let's do max
[04:14:46] pulling with f=2, s=2 again.
[04:19:00] So you could probably guess the output of this, f=2,
[04:23:45] s=2, this should reduce the height and
[04:26:76] width by a factor of 2, so you're left with 5 x 5 x 10.
[04:31:42] And so I'm going to call this Pool 2, and
[04:34:77] in our convention this is Layer 2 of the neural network.
[04:39:65] Now let's apply another convolutional layer to this.
[04:42:29] I'm going to use a 5 x 5 filter, so f = 5, and let's try this,
[04:47:11] 1, and I don't write the padding, means there's no padding.
[04:51:96] And this will give you the Conv 2 output, and that's your 16 filters.
[04:58:25] So this would be a 10 x 10 x 16 dimensional output.
[05:03:86] So we look at that, and this is the Conv 2 layer.
[05:10:38] And then let's apply max pooling to this with f=2, s=2.
[05:17:35] You can probably guess the output of this,
[05:20:22] we're at 10 x 10 x 16 with max pooling with f=2, s=2.
[05:24:55] This will half the height and
[05:26:66] width, you can probably guess the result of this, right?
[05:31:07] Left pooling with f = 2, s = 2.
[05:32:46] This should halve the height and width so you end up with
[05:37:66] a 5 x 5 x 16 volume, same number of channels as before.
[05:43:21] We're going to call this Pool 2.
[05:47:16] And in our convention this is Layer 2 because this
[05:52:42] has one set of weights and your Conv 2 layer.
[05:57:20] Now 5 x 5 x 16, 5 x 5 x 16 is equal to 400.
[06:03:30] So let's now fatten our Pool 2 into a 400 x 1 dimensional vector.
[06:10:89] So think of this as fatting this up into these set of neurons, like so.
[06:16:68] And what we're going to do is then take these 400 units and
[06:22:37] let's build the next layer, As having 120 units.
[06:30:07] So this is actually our first fully connected layer.
[06:33:24] I'm going to call this FC3 because we have
[06:38:39] 400 units densely connected to 120 units.
[06:46:24] So this fully connected unit, this fully connected layer is just like
[06:51:62] the single neural network layer that you saw in Courses 1 and 2.
[06:56:66] This is just a standard neural network where you have
[07:01:71] a weight matrix that's called W3 of dimension 120 x 400.
[07:08:04] And this is fully connected because each of the 400 units here is connected
[07:13:40] to each of the 120 units here, and you also have the bias parameter,
[07:18:35] yes that's going to be just a 120 dimensional, this is 120 outputs.
[07:23:65] And then lastly let's take 120 units and add another layer,
[07:28:71] this time smaller but let's say we had 84 units here,
[07:33:11] I'm going to call this fully connected Layer 4.
[07:36:88] And finally we now have 84 real numbers that you can fit to a [INAUDIBLE] unit.
[07:44:43] And if you're trying to do handwritten digital recognition,
[07:48:21] to recognize this hand it is 0, 1, 2, and so on up to 9.
[07:51:79] Then this would be a softmax with 10 outputs.
[07:56:68] So this is a vis-a-vis typical example of what
[08:00:96] a convolutional neural network might look like.
[08:05:48] And I know this seems like there a lot of hyper parameters.
[08:09:36] We'll give you some more specific suggestions later for
[08:12:91] how to choose these types of hyper parameters.
[08:15:88] Maybe one common guideline is to actually not try to invent your own
[08:20:38] settings of hyper parameters, but
[08:22:80] to look in the literature to see what hyper parameters you work for others.
[08:27:88] And to just choose an architecture that has worked well for
[08:30:96] someone else, and there's a chance that will work for your application as well.
[08:35:31] We'll see more about that next week.
[08:38:32] But for now I'll just point out that as you go deeper in the neural network,
[08:43:71] usually nh and nw to height and width will decrease.
[08:47:49] Pointed this out earlier, but it goes from 32 x 32, to 20 x 20, to 14 x 14,
[08:52:43] to 10 x 10, to 5 x 5.
[08:53:93] So as you go deeper usually the height and width will decrease,
[08:57:87] whereas the number of channels will increase.
[09:00:85] It's gone from 3 to 6 to 16, and then your fully connected layer is at the end.
[09:07:27] And another pretty common pattern you see in neural networks is to have conv layers,
[09:13:13] maybe one or more conv layers followed by a pooling layer, and
[09:17:42] then one or more conv layers followed by pooling layer.
[09:21:32] And then at the end you have a few fully connected layers and
[09:24:73] then followed by maybe a softmax.
[09:26:75] And this is another pretty common pattern you see in neural networks.
[09:32:37] So let's just go through for
[09:33:95] this neural network some more details of what are the activation shape,
[09:37:96] the activation size, and the number of parameters in this network.
[09:41:79] So the input was 32 x 30 x 3, and
[09:44:18] if you multiply out those numbers you should get 3,072.
[09:48:32] So the activation, a0 has dimension 3072.
[09:54:31] Well it's really 32 x 32 x 3.
[09:58:00] And there are no parameters I guess at the input layer.
[10:02:56] And as you look at the different layers,
[10:05:67] feel free to work out the details yourself.
[10:09:06] These are the activation shape and
[10:10:97] the activation sizes of these different layers.
[10:15:42] So just to point out a few things.
[10:16:95] First, notice that the max pooling layers don't have any parameters.
[10:23:35] Second, notice that the conv layers tend to have relatively
[10:28:20] few parameters, as we discussed in early videos.
[10:32:30] And in fact, a lot of the parameters tend to be in the fully
[10:36:41] collected layers of the neural network.
[10:39:42] And then you notice also that the activation size tends to
[10:44:58] maybe go down gradually as you go deeper in the neural network.
[10:50:28] If it drops too quickly, that's usually not great for performance as well.
[10:55:19] So it starts first there with 6,000 and 1,600, and
[11:00:34] then slowly falls into 84 until finally you have your Softmax output.
[11:06:40] You find that a lot of will have properties will
[11:10:68] have patterns similar to these.
[11:13:29] So you've now seen the basic building blocks of neural networks,
[11:16:45] your convolutional neural networks, the conv layer, the pooling layer,
[11:20:06] and the fully connected layer.
[11:21:60] A lot of computer division research has gone into figuring out how to put together
[11:25:69] these basic building blocks to build effective neural networks.
[11:29:07] And putting these things together actually requires quite a bit of insight.
[11:33:37] I think that one of the best ways for
[11:35:21] you to gain intuition is about how to put these things together is a C a number of
[11:39:32] concrete examples of how others have done it.
[11:41:80] So what I want to do next week is show you a few concrete examples even beyond this
[11:46:26] first one that you just saw on how people have successfully put these
[11:50:18] things together to build very effective neural networks.
[11:53:63] And through those videos next week l hope you hold your own intuitions about how
[11:58:53] these things are built.
[12:00:09] And as we are given concrete examples that architectures that maybe you can just use
[12:05:06] here exactly as developed by someone else or your own application.
[12:09:12] So we'll do that next week, but
[12:10:97] before wrapping this week's videos just one last thing which is one I'll talk
[12:15:49] a little bit in the next video about why you might want to use convolutions.
[12:19:84] Some benefits and
[12:20:86] advantages of using convolutions as well as how to put them all together.
[12:25:13] How to take a neural network like the one you just saw and actually train it
[12:29:02] on a training set to perform image recognition for some of the tasks.
[12:32:73] So with that let's go on to the last video of this week.
