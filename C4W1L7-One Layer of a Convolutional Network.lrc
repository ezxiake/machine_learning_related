[ti:]
[ar:]
[al:]
[by:¾Å¾ÅLrc¸è´ÊÍø¡«www.99Lrc.net]
[00:03:94] Get now ready to see how to build one layer of a convolutional neural network,
[00:07:66] let's go through the example.
[00:12:43] You've seen at the previous video how to take a 3D volume and
[00:17:14] convolve it with say two different filters.
[00:21:81] In order to get in this example to different 4 by 4 outputs.
[00:30:89] So let's say convolving with the first
[00:35:54] filter gives this first 4 by 4 output, and
[00:40:76] convolving with this second filter gives a different 4 by 4 output.
[00:49:10] The final thing to turn this into a convolutional neural net layer,
[00:55:57] is that for each of these we're going to add it bias,
[01:00:56] so this is going to be a real number.
[01:03:98] And where python broadcasting, you kind of have to add the same number so
[01:08:84] every one of these 16 elements.
[01:11:75] And then apply a non-linearity which for this illustration that says relative
[01:16:80] non-linearity, and this gives you a 4 by 4 output, all right?
[01:23:43] After applying the bias and the non-linearity.
[01:27:29] And then for this thing at the bottom as well, you add some different bias, again,
[01:31:89] this is a real number.
[01:33:15] So you add the single number to all 16 numbers, and
[01:36:47] then apply some non-linearity, let's say a real non-linearity.
[01:40:93] And this gives you a different 4 by 4 output.
[01:47:36] Then same as we did before, if we take this and
[01:52:46] stack it up as follows, so we ends up with a 4 by 4 by 2 outputs.
[01:59:69] Then this computation where you come from a 6 by 6 by 3 to 4 by 4 by 4,
[02:06:32] this is one layer of a convolutional neural network.
[02:11:30] So to map this back to one layer of four propagation in the standard neural
[02:15:63] network, in a non-convolutional neural network.
[02:18:83] Remember that one step before the prop was something like this, right?
[02:23:15] z1 = w1 times a0, a 0 was also equal to x,
[02:28:46] and then plus b[1].
[02:31:26] And you apply the non-linearity to get a[1], so that's g(z[1]).
[02:38:12] So this input here, in this analogy this is a[0], this is x3.
[02:44:77] And these filters here,
[02:48:07] this plays a role similar to w1.
[02:52:48] And you remember during the convolution operation, you were taking
[02:56:37] these 27 numbers, or really well, 27 times 2, because you have two filters.
[03:01:14] You're taking all of these numbers and multiplying them.
[03:03:90] So you're really computing a linear function to get this 4 x 4 matrix.
[03:09:63] So that 4 x 4 matrix, the output of the convolution operation,
[03:15:04] that plays a rolesimilar to w1 times a0.
[03:19:24] That's really maybe the output of this 4 x 4 as well as that 4 x 4.
[03:25:34] And then the other thing you do is add the bias.
[03:29:59] So, this thing here before applying value,
[03:35:13] this plays a role similar to z.
[03:38:93] And then it's finally by applying the non-linearity, this kind of this I guess.
[03:43:84] So, this output plays a role,
[03:49:74] this really becomes your activation at the next layer.
[03:53:39] So this is how you go from a0 to a1, as far as tthe linear operation and
[03:58:79] then convolution has all these multipled.
[04:02:19] So the convolution is really applying a linear operation and
[04:05:50] you have the biases and the applied value operation.
[04:08:43] And you've gone from a 6 by 6 by 3, dimensional a0,
[04:14:10] through one layer of neural network to,
[04:18:21] I guess a 4 by 4 by 2 dimensional a(1).
[04:22:69] And so 6 by 6 by 3 has gone to 4 by 4 by 2, and so
[04:27:14] that is one layer of convolutional net.
[04:33:69] Now in this example we have two filters, so we had two features of you will,
[04:40:50] which is why we wound up with our output 4 by 4 by 2.
[04:45:27] But if for example we instead had 10 filters instead of 2,
[04:49:24] then we would have wound up with the 4 by 4 by 10 dimensional output volume.
[04:54:39] Because we'll be taking 10 of these naps not just two of them, and stacking them
[05:00:33] up to form a 4 by 4 by 10 output volume, and that's what a1 would be.
[05:05:59] So, to make sure you understand this, let's go through an exercise.
[05:09:63] Let's suppose you have 10 filters, not just two filters, that are 3 by 3 by 3 and
[05:14:65] 1 layer of a neural network, how many parameters does this layer have?
[05:21:00] Well, let's figure this out.
[05:22:98] Each filter, is a 3 x 3 x 3 volume, so 3 x 3 x 3,
[05:29:43] so each fill has 27 parameters, all right?
[05:35:31] There's 27 numbers to be run, and plus the bias.
[05:42:21] So that was the b parameter, so this gives you 28 parameters.
[05:50:12] And then if you imagine that on the previous slide we had drawn two filters,
[05:54:45] but now if you imagine that you actually have ten of these, right?
[05:58:32] 1, 2..., 10 of these,
[06:01:74] then all together you'll have 28 times 10,
[06:06:94] so that will be 280 parameters.
[06:10:75] Notice one nice thing about this, is that no matter how big the input image is,
[06:16:31] the input image could be 1,000 by 1,000 or 5,000 by 5,000,
[06:22:05] but the number of parameters you have still remains fixed as 280.
[06:26:97] And you can use these ten filters to detect features, vertical edges,
[06:31:45] horizontal edges maybe other features anywhere even in a very,
[06:35:48] very large image is just a very small number of parameters.
[06:40:92] So these is really one property of convolution neural network that
[06:44:41] makes less prone to overfitting then if you could.
[06:48:00] So once you've learned 10 feature detectors that work,
[06:51:45] you could apply this even to large images.
[06:54:77] And the number of parameters still is fixed and relatively small,
[06:58:30] as 280 in this example.
[07:00:49] All right, so to wrap up this video let's just summarize the notation we
[07:05:13] are going to use to describe one layer to describe a covolutional layer in
[07:09:76] a convolutional neural network.
[07:11:98] So layer l is a convolution layer,
[07:14:30] l am going to use f superscript,[l] to denote the filter size.
[07:18:55] So previously we've been seeing the filters are f by f, and
[07:23:21] now this superscript square bracket l just denotes that this is
[07:28:16] a filter size of f by f filter layer l.
[07:31:07] And as usual the superscript square bracket l is the notation we're using to
[07:35:76] refer to particular layer l.
[07:39:81] going to use p[l] to denote the amount of padding.
[07:42:80] And again, the amount of padding can also be specified just by saying that you want
[07:47:36] a valid convolution, which means no padding, or
[07:50:13] a same convolution which means you choose the padding.
[07:53:24] So that the output size has the same height and width as the input size.
[07:59:00] And then you're going to use s[l] to denote the stride.
[08:03:25] Now, the input to this layer is going to be some dimension.
[08:09:45] It's going be some n by n by number of channels in the previous layer.
[08:18:59] Now, I'm going to modify this notation a little bit.
[08:21:16] I'm going to us superscript l- 1,
[08:25:38] because that's the activation from
[08:29:90] the previous layer, l- 1 times nc of l- 1.
[08:35:59] And in the example so far, we've been just using images of the same height and width.
[08:40:96] That in case the height and width might differ,
[08:43:99] l am going to use superscript h and superscript w, to denote the height and
[08:48:52] width of the input of the previous layer, all right?
[08:51:94] So in layer l, the size of the volume will be nh
[08:56:41] by nw by nc with superscript squared bracket l.
[09:01:13] It's just in layer l, the input to this layer Is whatever you had for
[09:05:59] the previous layer, so that's why you have l- 1 there.
[09:09:45] And then this layer of the neural network will itself output the value.
[09:16:73] So that will be nh of l by nw of l, by nc of l,
[09:23:06] that will be the size of the output.
[09:28:49] And so whereas we approve this set that the output volume size or
[09:34:94] at least the height and weight is given by this formula,
[09:40:65] n + 2p- f over s + 1, and then take the full of that and round it down.
[09:47:97] In this new notation what we have is that the outputs value that's in layer l,
[09:55:60] is going to be the dimension from the previous layer,
[10:00:89] plus the padding we're using in this layer l,
[10:05:47] minus the filter size we're using this layer l and so on.
[10:11:76] And technically this is true for the height, right?
[10:16:58] So the height of the output volume is given by this, and you can compute it
[10:21:51] with this formula on the right, and the same is true for the width as well.
[10:24:68] So you cross out h and
[10:26:67] throw in w as well, then the same formula with either the height or
[10:30:78] the width plugged in for computing the height or width of the output value.
[10:36:57] So that's how nhl -1 relates to nhl and wl- 1 relates to nwl.
[10:44:02] Now, how about the number of channels, where did those numbers come from?
[10:48:10] Let's take a look, if the output volume has this depth,
[10:52:78] while we know from the previous examples that that's equal
[10:57:66] to the number of filters we have in that layer, right?
[11:02:16] So we had two filters, the output value was 4 by 4 by 2, was 2 dimensional.
[11:07:01] And if you had 10 filters and your upper volume was 4 by 4 by 10.
[11:11:09] So, this the number of channels in the output value,
[11:15:74] that's just the number of filters we're using in this layer of the neural network.
[11:23:09] Next, how about the size of this filter?
[11:26:84] Well, each filter is going to be fl by fl by 100 number, right?
[11:33:05] So what is this last number?
[11:34:70] Well, we saw that you needed to convolve a 6 by 6 by 3 image,
[11:39:46] with a 3 by 3 by 3 filter.
[11:43:07] And so the number of channels in your filter, must match the number of channels
[11:48:15] in your input, so this number should match that number, right?
[11:54:36] Which is why each filter is going to be f(l) by f(l) by nc(l-1).
[12:02:62] And the output of this layer often apply devices in non-linearity,
[12:07:87] is going to be the activations of this layer al.
[12:11:74] And that we've already seen will be this dimension, right?
[12:15:11] The al will be a 3D volume,
[12:20:45] that's nHl by nwl by ncl.
[12:25:55] And when you are using a vectorized implementation or batch gradient
[12:31:08] descent or mini batch gradient descent, then you actually outputs Al,
[12:36:89] which is a set of m activations, if you have m examples.
[12:41:38] So that would be M by nHl, by nwl by ncl right?
[12:48:27] If say you're using bash grading decent and
[12:51:37] in the programming sizes this will be ordering of the variables.
[12:55:99] And we have the index and the trailing examples first,
[12:59:96] and then these three variables.
[13:02:38] Next how about the weights or the parameters, or kind of the w parameter?
[13:07:61] Well we saw already what the filter dimension is.
[13:10:26] So the filters are going to be f[l] by f[l] by nc [l- 1],
[13:16:25] but that's the dimension of one filter.
[13:20:23] How many filters do we have?
[13:22:24] Well, this is a total number of filters,
[13:25:09] the weights really all of the filters put together will have dimension given
[13:30:02] by this, times the total number of filters, right?
[13:33:51] Because this, Last quantity is the number of
[13:38:62] filters, In layer l.
[13:45:68] And then finally, you have the bias parameters, and
[13:48:71] you have one bias parameter, one real number for each filter.
[13:54:10] So you're going to have, the bias will have this many variables,
[13:57:97] it's just a vector of this dimension.
[14:00:81] Although later on we'll see that the code will be
[14:05:05] more convenient represented as 1 by 1 by 1 by nc[l]
[14:09:81] four dimensional matrix, or four dimensional tensor.
[14:16:43] So I know that was a lot of notation, and
[14:19:40] this is the convention I'll use for the most part.
[14:23:30] I just want to mention in case you search online and look at open source code.
[14:27:49] There isn't a completely universal standard convention about the ordering of
[14:32:31] height, width, and channel.
[14:34:26] So If you look on source code on GitHub or these open source implementations,
[14:39:14] you'll find that some authors use this order instead, where you first put
[14:43:87] the channel first, and you sometimes see that ordering of the variables.
[14:48:63] And in fact in some common frameworks, actually in multiple common frameworks,
[14:52:15] there's actually a variable or a parameter.
[14:54:15] Why do you want to list the number of channels first, or
[14:57:65] list the number of channels last when indexing into these volumes.
[15:02:00] I think both of these conventions work okay, so long as you're consistent.
[15:08:13] And unfortunately maybe this is one piece of annotation where
[15:13:04] there isn't consensus in the deep learning literature but
[15:17:77] i'm going to use this convention for these videos.
[15:24:68] Where we list height and width and then the number of channels last.
[15:30:76] So I know there was certainly a lot of new notations you could use, but you're
[15:34:32] thinking wow, that's a long notation, how do I need to remember all of these?
[15:38:02] Don't worry about it, you don't need to remember all of this notation, and
[15:41:99] through this week's exercises you become more familiar with it at that time.
[15:46:03] But the key point I hope you take a way from this video,
[15:49:11] is just one layer of how convolutional neural network works.
[15:52:69] And the computations involved in taking the activations of one layer and
[15:57:00] mapping that to the activations of the next layer.
[16:00:05] And next, now that you know how one layer of the compositional neural network works,
[16:04:06] let's stack a bunch of these together to actually form a deeper compositional
[16:07:74] neural network.
[16:09:04] Let's go on to the next video to see,
